{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+KewuTSzB5m3meEJlqcaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-iyi/llm-examples/blob/main/Google_Generative_AI_%26_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WumX1dpCXrKm",
        "outputId": "70f4bd83-5ec5-48ff-dc91-ea6fa84c69a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai langchain-google-genai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Load secrets from colab\n",
        "GOOGLE_API_KEY = userdata.get('gemini')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Add to environment variables.\n",
        "!echo 'GOOGLE_API_KEY='{GOOGLE_API_KEY} > .env\n",
        "!echo 'OPENAI_API_KEY='{OPENAI_API_KEY} >> .env"
      ],
      "metadata": {
        "id": "cmCCUD-ZX4Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHFBlDIhYQ6_",
        "outputId": "581b1d6c-c6c2-40a4-f010-f9c2e5545e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".  ..  .config\t.env  res  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM6C2PoUYa4e",
        "outputId": "f8ebdae6-6f5d-4f5f-fc92-e05ac84f3c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text: str) -> Markdown:\n",
        "  \"\"\"Convert model output into Markdown for easy display.\"\"\"\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "cfFjRjWAY0nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Google GenerativeAI API Key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "yQoL49BAZMSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name='gemini-pro')\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb_Ejvi6c55R",
        "outputId": "f3391524-2a0b-4d23-deb9-c453e5271365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "    'What is Mixture of Experts?',\n",
        "    'What is so special a'\n",
        "]\n",
        "\n",
        "response = model.generate_content(prompt)"
      ],
      "metadata": {
        "id": "P_Q1yXjjdI-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "atiwlVDIdpDG",
        "outputId": "89eaba36-907b-415a-a707-b89254eec547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **Mixture of Experts (MoE)**\n> \n> **Definition:**\n> Mixture of Experts is a machine learning model that consists of multiple expert networks or sub-models, each of which is specialized in handling a specific subset of data or problem. The overall prediction of the MoE model is a weighted average of the predictions from the individual experts.\n> \n> **Special Features:**\n> \n> * **Modularity:** MoE models are highly modular, allowing different experts to be added or removed as needed, making them flexible and adaptable to changing data or tasks.\n> * **Specialization:** Each expert is trained on a specific subset of data, enabling it to learn more specialized knowledge for that subset.\n> * **Parallel Computation:** The expert networks can be executed in parallel, which can significantly improve training and inference speed on large datasets.\n> * **Robustness:** MoE models are more robust to outliers and noise in the data because the experts can compensate for each other's weaknesses.\n> * **Interpretability:** The expert networks provide a way to understand how the model makes decisions, which can be useful for debugging and diagnostics.\n> \n> **How It Works:**\n> \n> 1. **Data Partitioning:** The training data is split into subsets, each corresponding to the expertise of one expert network.\n> 2. **Expert Training:** Each expert network is trained independently on its assigned subset of data.\n> 3. **Gating Network:** A gating network learns to determine which expert is most appropriate for each data point.\n> 4. **Prediction:** The predictions from the individual experts are combined using weights determined by the gating network.\n> \n> **Applications:**\n> \n> MoE models are used in various applications, including:\n> \n> * Image classification\n> * Natural language processing\n> * Audio and video analysis\n> * Recommender systems\n> * Time series forecasting\n> \n> **Advantages:**\n> \n> * Improved accuracy and performance\n> * Reduced overfitting\n> * Enhanced interpretability\n> * Suitable for large-scale datasets\n> * Faster training and inference times\n> \n> **Disadvantages:**\n> \n> * Increased model complexity\n> * Potential for overspecialization\n> * Requires careful design and implementation"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Google GenerativeAI with LangChain"
      ],
      "metadata": {
        "id": "ithFhlMRd7R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "yaD15Oqqdrri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model='gemini-pro')\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIozjHS7eDku",
        "outputId": "cbbd55c6-aa5a-4a62-cdd0-fbb3c4e11e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='gemini-pro', client=genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(prompt[0])"
      ],
      "metadata": {
        "id": "5AWpzsrReJy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "_sKV2xj2ePy8",
        "outputId": "c91f4c4c-8291-4cd1-bd5f-2259df369cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **Mixture of Experts (MoE)** is a machine learning technique that leverages multiple specialized sub-models, known as experts, to make predictions. It is primarily used in the context of neural networks.\n> \n> **How MoE Works:**\n> \n> 1. **Input Data:** The input data is passed through a gating network.\n> \n> 2. **Gating Network:** The gating network assigns weights (probabilities) to each expert. These weights determine the level of influence each expert will have on the final prediction.\n> \n> 3. **Expert Models:** Multiple expert models are simultaneously trained on different subsets of the data or different aspects of the problem. Each expert is specialized in handling specific tasks or features.\n> \n> 4. **Expert Predictions:** Each expert model makes predictions for the input data.\n> \n> 5. **Mixture:** The predictions from all the experts are combined using the weighted average, where the weights are the probabilities assigned by the gating network.\n> \n> **Benefits of MoE:**\n> \n> * **Improved Performance:** By combining the expertise of multiple specialized models, MoE can achieve higher accuracy and generalization capabilities.\n> * **Reduced Overfitting:** The gating network helps prevent overfitting by selectively activating experts that are relevant to the input data.\n> * **Scalability:** MoE can be scaled to handle large datasets and complex tasks by adding more expert models.\n> * **Explainability:** The gating network provides insights into which experts contribute to the final prediction, making MoE more interpretable.\n> \n> **Applications of MoE:**\n> \n> * Image classification\n> * Natural language processing\n> * Speech recognition\n> * Recommendation systems\n> * Fraud detection\n> \n> **Advantages of MoE:**\n> \n> * Can handle complex tasks with multiple aspects\n> * Improves performance through specialization\n> * Scales well to large datasets\n> * Provides explainability\n> \n> **Limitations of MoE:**\n> \n> * Can be computationally expensive to train\n> * Requires careful selection and training of experts\n> * May suffer from cold start issues if new data differs significantly from the training data"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the GeminiPro Vision API"
      ],
      "metadata": {
        "id": "sjbVJv5ef02J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-pro-vision')\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xed_RDi8fh4q",
        "outputId": "ea4e3cb7-4230-4b6c-c13c-969e34748e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='gemini-pro-vision', client=genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro-vision',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            'type': 'text',\n",
        "            'text': 'What do you see in this image?',\n",
        "        },\n",
        "        {\n",
        "            'type': 'image_url',\n",
        "            'image_url': 'https://www.goconstruct.org/media/qukhn3dc/soh.jpg?anchor=center&mode=crop&width=940&height=610&rnd=132743881555030000',\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6FaTg9ogPdP",
        "outputId": "cda27ba0-08d5-4538-9dc2-2c1aa62780c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HumanMessage(content=[{'type': 'text', 'text': 'What do you see in this image?'}, {'type': 'image_url', 'image_url': 'https://www.goconstruct.org/media/qukhn3dc/soh.jpg?anchor=center&mode=crop&width=940&height=610&rnd=132743881555030000'}])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke([message])\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAOKWOVPgnF6",
        "outputId": "8110cbc1-8eb8-4684-fab1-28049b65a2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=' The Sydney Opera House.', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-e7e3f4de-8330-44c0-8f7a-e85313f42f5b-0')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1H5zTYXXhIak",
        "outputId": "49946c04-d04e-4e4e-f244-700dde5e55ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The Sydney Opera House.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\n",
        "            'type': 'text',\n",
        "            'text': 'What do you see in this image? Write a short article about the content of the image.',\n",
        "        },\n",
        "        {\n",
        "            'type': 'image_url',\n",
        "            'image_url': 'https://www.goconstruct.org/media/qukhn3dc/soh.jpg?anchor=center&mode=crop&width=940&height=610&rnd=132743881555030000',\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVGbvktChPkC",
        "outputId": "feb54caf-650a-4861-dee2-101022d2e770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HumanMessage(content=[{'type': 'text', 'text': 'What do you see in this image? Write a short article about the content of the image.'}, {'type': 'image_url', 'image_url': 'https://www.goconstruct.org/media/qukhn3dc/soh.jpg?anchor=center&mode=crop&width=940&height=610&rnd=132743881555030000'}])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke([message])\n",
        "to_markdown(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "Ik3FUXXZkoVR",
        "outputId": "1f01d694-7c02-44f7-9bf8-307357470408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">  The Sydney Opera House is one of the most iconic buildings in the world. It is located in Sydney, Australia, and was designed by Danish architect Jørn Utzon. The building was completed in 1973 and is a UNESCO World Heritage Site.\n> \n> The Sydney Opera House is known for its unique design, which features a series of large, white shells. The shells are made of precast concrete and are supported by a steel frame. The building is also known for its acoustics, which are considered to be some of the best in the world.\n> \n> The Sydney Opera House is home to the Sydney Symphony Orchestra, the Australian Ballet, and the Australian Opera. It also hosts a variety of other events, such as concerts, plays, and exhibitions. The Sydney Opera House is a popular tourist destination and is visited by millions of people each year."
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In Contenxt Retrieval"
      ],
      "metadata": {
        "id": "LsPxHBTfl4V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0.3)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Stxdi6Akz35",
        "outputId": "ccbd7bfe-8598-4213-baa8-eeb7a11454a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='gemini-pro', temperature=0.3, client=genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the document.\n",
        "from pathlib import Path\n",
        "import urllib\n",
        "\n",
        "# Create data folder\n",
        "data_folder = Path.cwd() / 'res/data'\n",
        "Path(data_folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create pdf file path.\n",
        "pdf_url = 'https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf'\n",
        "pdf_file = str(Path(data_folder, pdf_url.split('/')[-1]))\n",
        "\n",
        "# Download pdf\n",
        "urllib.request.urlretrieve(pdf_url, pdf_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkjz2D3XmCWj",
        "outputId": "9c25ddc5-b1eb-4a76-f379-85bc9df3b5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/res/data/practitioners_guide_to_mlops_whitepaper.pdf',\n",
              " <http.client.HTTPMessage at 0x7b9de9b1ab00>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain langchain-community langchain-text-splitters\n",
        "!pip install -q -U pypdf tiktoken chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkOdmAaWmdo1",
        "outputId": "34a8068b-c75d-4700-e5fe-e0dae831b33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/526.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/526.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "j3g3WyznnlVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9BcC3j5wpPyJ",
        "outputId": "bd2ce2d9-5607-4904-c857-2dd6141f93c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/res/data/practitioners_guide_to_mlops_whitepaper.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PDF into Document objects\n",
        "pdf_loader = PyPDFLoader(pdf_file)\n",
        "pages = pdf_loader.load_and_split()\n",
        "print(pages[2].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU1wzxeGpEX5",
        "outputId": "f20c4998-1457-4a13-bcf8-262bd8df5d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executive summary\n",
            "Across industries, DevOps and DataOps have been widely adopted as methodologies to improve quality and re -\n",
            "duce the time to market of software engineering and data engineering initiatives. With the rapid growth in machine \n",
            "learning (ML) systems, similar approaches need to be developed in the context of ML engineering, which handle the \n",
            "unique complexities of the practical applications of ML. This is the domain of MLOps. MLOps is a set of standard -\n",
            "ized processes and technology capabilities for building, deploying, and operationalizing ML systems rapidly and \n",
            "reliably.]\n",
            "We previously published Google Cloud’s AI Adoption Framework  to provide guidance for technology leaders who \n",
            "want to build an effective artificial intelligence (AI) capability in order to transform their business. That framework \n",
            "covers AI challenges around people, data, technology, and process, structured in six different themes: learn, lead, \n",
            "access, secure, scale, and automate . \n",
            "The current document takes a deeper dive into the themes of scale  and automate  to illustrate the requirements for \n",
            "building and operationalizing ML systems. Scale  concerns the extent to which you use cloud managed ML services \n",
            "that scale with large amounts of data and large numbers of data processing and ML jobs, with reduced operational \n",
            "overhead. Automate  concerns the extent to which you are able to deploy, execute, and operate technology for data \n",
            "processing and ML pipelines in production efficiently, frequently, and reliably.\n",
            "We outline an MLOps framework that defines core processes and technical capabilities. Organizations can use this \n",
            "framework to help establish mature MLOps practices for building and operationalizing ML systems. Adopting the \n",
            "framework can help organizations improve collaboration between teams, improve the reliability and scalability of ML \n",
            "systems, and shorten development cycle times. These benefits in turn drive innovation and help gain overall busi -\n",
            "ness value from investments in ML.\n",
            "This document is intended for technology leaders and enterprise architects who want to understand MLOps. It’s also \n",
            "for teams who want details about what MLOps looks like in practice. The document assumes that readers are famil -\n",
            "iar with basic machine learning concepts and with development and deployment practices such as CI/CD.\n",
            "The document is in two parts. The first part, an overview of the MLOps lifecycle, is for all readers. It introduces \n",
            "MLOps processes and capabilities and why they’re important for successful adoption of ML-based systems.\n",
            "The second part is a deep dive on the MLOps processes and capabilities. This part is for readers who want to un -\n",
            "derstand the concrete details of tasks like running a continuous training pipeline, deploying a model, and monitoring \n",
            "predictive performance of an ML model.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = '\\n'.join(str(p.page_content) for p in pages[:30])\n",
        "print(f'The total words in the context: {len(context):,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3AdpI6hpd0s",
        "outputId": "e3750935-3992-4ff3-a167-ef575706064c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total words in the context: 55,545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Design - In Context"
      ],
      "metadata": {
        "id": "Sg3817Bdsw7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n"
      ],
      "metadata": {
        "id": "iS4M_u8LtUb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = '''\\\n",
        "Answer the question as precise as possible using provided context. If the answer\n",
        "is not contained in the context, say \"answer not available in context\" \\n\\n\n",
        "Context: \\n{context}\\n\n",
        "Question: \\n{question}?\\n\n",
        "Answer:\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=['context', 'question'],\n",
        ")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BeXm8MCsgu8",
        "outputId": "1b24b8ed-dc6e-4f31-eb6f-a4f199c7f6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context', 'question'], template='Answer the question as precise as possible using provided context. If the answer\\nis not contained in the context, say \"answer not available in context\" \\n\\n\\nContext: \\n{context}\\n\\nQuestion: \\n{question}?\\n\\nAnswer:\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "bJv9RoUytlHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limited context\n",
        "stuff_chain = load_qa_chain(model, chain_type='stuff', prompt=prompt)\n",
        "stuff_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2Un5pLYwD6O",
        "outputId": "12facd33-de82-467f-fd9f-c265101cbaca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question as precise as possible using provided context. If the answer\\nis not contained in the context, say \"answer not available in context\" \\n\\n\\nContext: \\n{context}\\n\\nQuestion: \\n{question}?\\n\\nAnswer:\\n'), llm=ChatGoogleGenerativeAI(model='gemini-pro', temperature=0.3, client=genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "))), document_variable_name='context')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer is within pages[6:8]\n",
        "question = 'What is Experimentation? Provide a detailed answer.'\n",
        "\n",
        "stuff_answer = stuff_chain(\n",
        "    {\n",
        "        'input_documents': pages[6:8],  # Answer is within these pages.\n",
        "        'question': question,\n",
        "    },\n",
        "    return_only_outputs=True,\n",
        ")\n",
        "pprint(stuff_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sr8oE6GwKH-",
        "outputId": "de81e99b-e849-46f8-ae33-2072adddc561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'output_text': 'Experimentation is the core activity during the ML '\n",
            "                'development phase. Data scientists and ML researchers '\n",
            "                'prototype model architectures and training routines, create '\n",
            "                'labeled datasets, and use features and other reusable ML '\n",
            "                'artifacts that are governed through the data and model '\n",
            "                'management process.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer is NOT within pages[6:8]\n",
        "question = 'Describe data management and feature management systems.'\n",
        "\n",
        "stuff_answer = stuff_chain(\n",
        "    {\n",
        "        'input_documents': pages[6:8],  # Answer is within these pages.\n",
        "        'question': question,\n",
        "    },\n",
        "    return_only_outputs=True,\n",
        ")\n",
        "pprint(stuff_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctiyfNctwl8r",
        "outputId": "848aaaaf-1f0d-44a0-f01e-6eebbde743ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'output_text': 'Answer not available in context'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Pipeline: Embedding + LLM"
      ],
      "metadata": {
        "id": "ois4DH2Zy3TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores.chroma import Chroma"
      ],
      "metadata": {
        "id": "CyqPM1Orw9pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10_000, chunk_overlap=0)\n",
        "context = '\\n\\n'.join(str(p.page_content) for p in pages)\n",
        "texts = text_splitter.split_text(context)"
      ],
      "metadata": {
        "id": "WDChgqPkzh3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkEC4VgWzyJA",
        "outputId": "b47008eb-8b0c-41e6-987c-93bdd846d857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Practitioners guide to MLOps:  \\nA framework for continuous \\ndelivery and automation of  \\nmachine learning.White paper\\nMay 2021\\nAuthors:  \\nKhalid Salama,  \\nJarek Kazmierczak,  \\nDonna Schut\\n\\nTable of Contents\\nExecutive summary  3\\nOverview of MLOps lifecycle and core capabilities  4\\nDeep dive of MLOps processes  15\\nPutting it all together  34\\nAdditional resources  36Building an ML-enabled system  6\\nThe MLOps lifecycle  7\\nMLOps: An end-to-end workflow  8\\nMLOps capabilities  9\\n      Experimentation  11\\n      Data processing  11\\n      Model training  11\\n      Model evaluation  12\\n      Model serving  12\\n      Online experimentation  13\\n      Model monitoring  13\\n      ML pipelines  13\\n      Model registry  14\\n      Dataset and feature repository  14\\n      ML metadata and artifact tracking  15\\nML development  16\\nTraining operationalization  18\\nContinuous training  20\\nModel deployment  23\\nPrediction serving  25\\nContinuous monitoring  26\\nData and model management  29\\n      Dataset and feature management  29\\n             Feature management  30\\n             Dataset management  31\\n      Model management  32\\n             ML metadata tracking  32\\n             Model governance  33\\n\\nExecutive summary\\nAcross industries, DevOps and DataOps have been widely adopted as methodologies to improve quality and re -\\nduce the time to market of software engineering and data engineering initiatives. With the rapid growth in machine \\nlearning (ML) systems, similar approaches need to be developed in the context of ML engineering, which handle the \\nunique complexities of the practical applications of ML. This is the domain of MLOps. MLOps is a set of standard -\\nized processes and technology capabilities for building, deploying, and operationalizing ML systems rapidly and \\nreliably.]\\nWe previously published Google Cloud’s AI Adoption Framework  to provide guidance for technology leaders who \\nwant to build an effective artificial intelligence (AI) capability in order to transform their business. That framework \\ncovers AI challenges around people, data, technology, and process, structured in six different themes: learn, lead, \\naccess, secure, scale, and automate . \\nThe current document takes a deeper dive into the themes of scale  and automate  to illustrate the requirements for \\nbuilding and operationalizing ML systems. Scale  concerns the extent to which you use cloud managed ML services \\nthat scale with large amounts of data and large numbers of data processing and ML jobs, with reduced operational \\noverhead. Automate  concerns the extent to which you are able to deploy, execute, and operate technology for data \\nprocessing and ML pipelines in production efficiently, frequently, and reliably.\\nWe outline an MLOps framework that defines core processes and technical capabilities. Organizations can use this \\nframework to help establish mature MLOps practices for building and operationalizing ML systems. Adopting the \\nframework can help organizations improve collaboration between teams, improve the reliability and scalability of ML \\nsystems, and shorten development cycle times. These benefits in turn drive innovation and help gain overall busi -\\nness value from investments in ML.\\nThis document is intended for technology leaders and enterprise architects who want to understand MLOps. It’s also \\nfor teams who want details about what MLOps looks like in practice. The document assumes that readers are famil -\\niar with basic machine learning concepts and with development and deployment practices such as CI/CD.\\nThe document is in two parts. The first part, an overview of the MLOps lifecycle, is for all readers. It introduces \\nMLOps processes and capabilities and why they’re important for successful adoption of ML-based systems.\\nThe second part is a deep dive on the MLOps processes and capabilities. This part is for readers who want to un -\\nderstand the concrete details of tasks like running a continuous training pipeline, deploying a model, and monitoring \\npredictive performance of an ML model.3\\n\\n4\\nOrganizations can use the framework to identify gaps in building an integrated ML platform and to focus on the scale \\nand automate themes from Google’s AI Adoption Framework. The decision about whether (or to which degree) to \\nadopt each of these processes and capabilities in your organization depends on your business context. For exam -\\nple, you must determine the business value that the framework creates when compared to the cost of purchasing or \\nbuilding capabilities (for example, the cost in engineering hours).\\nOverview of MLOps lifecycle  \\nand core capabilities\\nDespite the growing recognition of AI/ML as a crucial pillar of digital transformation, successful deployments and \\neffective operations are a bottleneck for getting value from AI. Only one in two organizations has moved beyond \\npilots and proofs of concept. Moreover, 72% of a cohort of organizations that began AI pilots before 2019 have not \\nbeen able to deploy even a single application in production.1 Algorithmia’s survey of the state of enterprise machine \\nlearning found that 55% of companies surveyed have not deployed an ML model.2 To summarize: models don’t make \\nit into production, and if they do, they break because they fail to adapt to changes in the environment.\\nThis is due to a variety of issues. Teams engage in a high degree of manual and one-off work. They do not have reus -\\nable or reproducible components, and their processes involve difficulties in handoffs between data scientists and IT. \\nDeloitte identified lack of talent and integration issues as factors that can stall or derail AI initiatives.3 Algorithmia’s \\nsurvey highlighted that challenges in deployment, scaling, and versioning efforts still hinder teams from getting value \\nfrom their investments in ML. Capgemini Research noted that the top three challenges faced by organizations in \\nachieving deployments at scale are lack of mid- to senior-level talent, lack of change-management processes, and \\nlack of strong governance models for achieving scale.\\nThe common theme in these and other studies is that ML systems cannot be built in an ad hoc manner, isolated from \\nother IT initiatives like DataOps and DevOps. They also cannot be built without adopting and applying sound software \\nengineering practices, while taking into account the factors that make operationalizing ML different from operational -\\nizing other types of software.\\nOrganizations need an automated and streamlined ML process. This process does not just help the organization \\nsuccessfully deploy ML models in production. It also helps manage risk when organizations scale the number of \\nML applications to more use cases in changing environments, and it helps ensure that the applications are still in \\nline with business goals. McKinsey’s Global Survey on AI found that having standard frameworks and development \\n1 The AI-powered enterprise , CapGemini Research Institute, 2020.\\n2 2020 state of enterprise machine learning , Algorithmia, 2020.\\n3 Artificial intelligence for the real world , Deloitte, 2017.\\n4 The state of AI in 2020 , McKinsey, 2020.\\n\\n5\\nprocesses in place is one of the differentiating factors of high-performing ML teams.4\\nThis is where ML engineering can be essential. ML engineering is at the center of building ML-enabled systems, \\nwhich concerns the development and operationalizing of production-grade ML systems. ML engineering provides a \\nsuperset of the discipline of software engineering that handles the unique complexities of the practical applications \\nof ML.5 These complexities include the following:\\n• Preparing and maintaining high-quality data for training ML models.\\n• Tracking models in production to detect performance degradation.\\n• Performing ongoing experimentation of new data sources, ML algorithms, and hyperparameters, and then \\ntracking these experiments.\\n• Maintaining the veracity of models by continuously retraining them on fresh data.\\n• Avoiding training-serving skews that are due to inconsistencies in data and in runtime dependencies between \\ntraining environments and serving environments.\\n• Handling concerns about model fairness and adversarial attacks.\\nMLOps is a methodology for ML engineering that unifies ML system development (the ML element) with ML system \\noperations (the Ops element). It advocates formalizing and (when beneficial) automating critical steps of ML system \\nconstruction. MLOps provides a set of standardized processes and technology capabilities for building, deploying, \\nand operationalizing ML systems rapidly and reliably.\\nMLOps supports ML development and deployment in the way that DevOps and DataOps support application engi -\\nneering and data engineering (analytics). The difference is that when you deploy a web service, you care about resil -\\nience, queries per second, load balancing, and so on. When you deploy an ML model, you also need to worry about \\nchanges in the data, changes in the model, users trying to game the system, and so on. This is what MLOps is about.\\nMLOps practices can result in the following benefits over systems that do not follow MLOps practices:\\n• Shorter development cycles, and as a result, shorter time to market.\\n• Better collaboration between teams.\\n• Increased reliability, performance, scalability, and security of ML systems. \\n• Streamlined operational and governance processes. \\n• Increased return on investment of ML projects.\\nIn this section, you learn about the MLOps lifecycle and workflow, and about the individual capabilities that are re -\\n5 Towards ML Engineering , Google, 2020.',\n",
              " '6\\nquired for a robust MLOps implementation.\\nBuilding an ML-enabled system\\nBuilding an ML-enabled system is a multifaceted undertaking that combines data engineering, ML engineering, and \\napplication engineering tasks, as shown in figure 1.\\nData engineering involves ingesting, integrating, curating, and refining data to facilitate a broad spectrum of opera -\\ntional tasks, data analytics tasks, and ML tasks. Data engineering can be crucial to the success of the analytics and \\nML initiatives. If an organization does not have robust data engineering processes and technologies, it might not be \\nset up for success with downstream business intelligence, advanced analytics, or ML projects.\\nML models are built and deployed in production using curated data that is usually created by the data engineering \\nteam. The models do not operate in silos; they are components of, and support, a large range of application systems, \\nsuch as business intelligence systems, line of business applications, process control systems, and embedded sys -\\ntems. Integrating an ML model into an application is a critical task that involves making sure first that the deployed \\nmodel is used effectively by the applications, and then monitoring model performance. In addition to this, you should \\nalso collect and monitor relevant business KPIs (for example, click-through rate, revenue uplift, and user experience). \\nThis information helps you understand the impact of the ML model on the business and adapt accordingly.\\nFigure 1.  The relationship of data engineering, ML engineering, and app engineering\\n\\n7\\nThe MLOps lifecycle\\nThe MLOps lifecycle encompasses seven integrated and iterative processes, as shown in figure 2.\\nThe processes can consist of the following:\\n• ML development  concerns experimenting and developing a robust and reproducible model training proce -\\ndure (training pipeline code), which consists of multiple tasks from data preparation and transformation to \\nmodel training and evaluation.\\n• Training operationalization  concerns automating the process of packaging, testing, and deploying repeat -\\nable and reliable training pipelines.\\n• Continuous training  concerns repeatedly executing the training pipeline in response to new data or to code \\nchanges, or on a schedule, potentially with new training settings.\\n• Model deployment  concerns packaging, testing, and deploying a model to a serving environment for online \\nexperimentation and production serving.\\nFigure 2 . The MLOps lifecycle\\n\\n8\\n• Prediction serving  is about serving the model that is deployed in production for inference.\\n• Continuous monitoring  is about monitoring the effectiveness and efficiency of a deployed model.\\n• Data and model management  is a central, cross-cutting function for governing ML artifacts to support audit -\\nability, traceability, and compliance. Data and model management can also promote shareability, reusability, \\nand discoverability of ML assets.\\nMLOps: An end-to-end workflow\\nFigure 3 shows a simplified but canonical flow for how the MLOps processes interact with each other, focusing on \\nhigh-level flow of control and on key inputs and outputs.\\nThis is not a waterfall workflow that has to sequentially pass through all the processes. The processes can be \\nskipped, or the flow can repeat a given phase or a subsequence of the processes. The diagram shows the following \\nflow:\\n1. The core activity during this ML development phase is experimentation. As data scientists and ML research -\\ners prototype model architectures and training routines, they create labeled datasets, and they use features \\nand other reusable ML artifacts that are governed through the data and model management process. The \\nFigure 3.  The MLOps process\\n\\n9\\nprimary output of this process is a formalized training procedure, which includes data preprocessing, model \\narchitecture, and model training settings. \\n2. If the ML system requires continuous training (repeated retraining of the model), the training procedure is \\noperationalized as a training pipeline. This requires a CI/CD routine to build, test, and deploy the pipeline to \\nthe target execution environment.\\n3. The continuous training pipeline is executed repeatedly based on retraining triggers, and it produces a model \\nas output. The model is retrained as new data becomes available, or if model performance decay is detected. \\nOther training artifacts and metadata that are produced by a training pipeline are also tracked. If the pipeline \\nproduces a successful model candidate, that candidate is then tracked by the model management process \\nas a registered model. \\n4. The registered model is annotated, reviewed, and approved for release and is then deployed to a production \\nenvironment. This process might be relatively opaque if you are using a no-code solution, or it can involve \\nbuilding a custom CI/CD pipeline for progressive delivery. \\n5. The deployed model serves predictions using the deployment pattern that you have specified: online, batch, \\nor streaming predictions. In addition to serving predictions, the serving runtime can generate model explana -\\ntions and capture serving logs to be used by the continuous monitoring process. \\n6. The continuous monitoring process monitors the model for predictive effectiveness and service. The primary \\nconcern of effectiveness performance monitoring is detecting model decay—for example, data and concept \\ndrift. The model deployment can also be monitored for efficiency metrics like latency, throughput, hardware \\nresource utilization, and execution errors. \\nMLOps capabilities\\nTo effectively implement the key MLOps processes outlined in the previous section, organizations need to establish a \\nset of core technical capabilities. These capabilities can be provided by a single integrated ML platform. Alternative -\\nly, they can be created by combining vendor tools that each are best suited to particular tasks, developed as custom \\nservices, or created as a combination of these approaches.\\nIn most cases, the processes are deployed in stages rather than all at once in a single deployment. An organization’s \\nplan for adopting these processes and capabilities should align with business priorities and with the organization’s \\ntechnical and skills maturity. For example, many organizations start by focusing on the processes for ML develop -\\nment, model deployment, and prediction serving. For these organizations, continuous training and continuous moni -\\ntoring might not be necessary if they are piloting a relatively small number of ML systems.\\nFigure 4 shows the core set of technical capabilities that are generally required for MLOps. They are abstracted as \\nfunctional components that can have many-to-many mappings to specific products and technologies.\\n\\n10\\nSome foundational capabilities are required in order to support any IT workload, such as a reliable, scalable, and \\nsecure compute infrastructure. Most organizations already have investments in these capabilities and can benefit by \\ntaking advantage of them for ML workflows. Such capabilities might span multiple clouds, or even operate partially \\non-premises. Ideally, this would include advanced capabilities such as specialized ML accelerators.\\nIn addition, an organization needs standardized configuration management and CI/CD capabilities to build, test, \\nrelease, and operate software systems rapidly and reliably, including ML systems.\\nOn top of these foundational capabilities is a set of core MLOps capabilities. These include experimentation, data \\nprocessing, model training, model evaluation, model serving, online experimentation, model monitoring, ML pipeline, \\nand model registry. Finally, two cross-cutting capabilities that enable integration and interaction are an ML metadata \\nand artifact repository and an ML dataset and feature repository.\\nFigure 4.  Core MLOps technical capabilities\\n\\n11\\nThe following sections outline the characteristics of each of the MLOps capabilities.\\nExperimentation \\nThe experimentation capability lets your data scientists and ML researchers collaboratively perform exploratory data \\nanalysis, create prototype model architectures, and implement training routines. An ML environment should also let \\nthem write modular, reusable, and testable source code that is version controlled. Key functionalities in experimenta -\\ntion include the following:\\n• Provide notebook environments that are integrated with version control tools like Git.\\n• Track experiments, including information about the data, hyperparameters, and evaluation metrics for  \\nreproducibility and comparison.\\n• Analyze and visualize data and models.\\n• Support exploring datasets, finding experiments, and reviewing implementations.\\n• Integrate with other data services and ML services in your platform.\\nData processing\\nThe data processing capability lets you prepare and transform large amounts of data for ML at scale in ML develop -\\nment, in continuous training pipelines, and in prediction serving. Key functionalities in data processing include the \\nfollowing:\\n• Support interactive execution (for example, from notebooks) for quick experimentation and for long-running \\njobs in production.\\n• Provide data connectors to a wide range of data sources and services, as well as data encoders and  \\ndecoders for various data structures and formats.\\n• Provide both rich and efficient data transformations and ML feature engineering for structured (tabular) and \\nunstructured data (text, image, and so on).\\n• Support scalable batch and stream data processing for ML training and serving workloads.\\nModel training\\nThe model training capability lets you efficiently and cost-effectively run powerful algorithms for training ML models.',\n",
              " '12\\nModel training should be able to scale with the size of both the models and the datasets that are used for training. \\nKey functionalities in model training include the following:\\n• Support common ML frameworks and support custom runtime environments.\\n• Support large-scale distributed training with different strategies for multiple GPUs and multiple workers.\\n• Enable on-demand use of ML accelerators.\\n• Allow efficient hyperparameter tuning and target optimization at scale.\\n• Ideally, provide built-in automated ML (AutoML) functionality, including automated feature selection and engi -\\nneering as well as automated model architecture search and selection.\\nModel evaluation\\nThe model evaluation capability lets you assess the effectiveness of your model, interactively during experimentation \\nand automatically in production. Key functionalities in model evaluation include the following:\\n• Perform batch scoring of your models on evaluation datasets at scale.\\n• Compute pre-defined or custom evaluation metrics for your model on different slices of the data.\\n• Track trained-model predictive performance across different continuous-training executions.\\n• Visualize and compare performances of different models.\\n• Provide tools for what-if analysis and for identifying bias and fairness issues.\\n• Enable model behavior interpretation using various explainable AI techniques.\\nModel serving\\nThe model serving capability lets you deploy and serve your models in production environments. Key functionalities \\nin model serving include the following:\\n• Provide support for low-latency, near-real-time (online) prediction and high-throughput batch (offline)  \\nprediction.\\n• Provide built-in support for common ML serving frameworks (for example, TensorFlow Serving , TorchServe , \\nNvidia Triton , and others for Scikit-learn  and XGBoost  models) and for custom runtime environments.\\n• Enable composite prediction routines, where multiple models are invoked hierarchically or simultaneously \\nbefore the results are aggregated, in addition to any required pre- or post-processing routines.\\n• Allow efficient use of ML inference accelerators with autoscaling to match spiky workloads and to balance\\n\\n13\\ncost with latency.\\n• Support model explainability using techniques like feature attributions for a given model prediction.\\n• Support logging of prediction serving requests and responses for analysis. \\nOnline experimentation\\nThe online experimentation capability lets you understand how newly trained models perform in production settings \\ncompared to the current models (if any) before you release the new model to production. For example, using a small \\nsubset of the serving population, you use online experimentation to understand the impact that a new recommen -\\ndation system has on click-throughs and on conversation rates. The results of online experimentation should be \\nintegrated with the model registry  capability to facilitate the decision about releasing the model to production. Online \\nexperimentation enhances the reliability of your ML releases by helping you decide to discard ill-performing models \\nand to promote well-performing ones. Key functionalities in online experimentation include the following:\\n• Support canary and shadow deployments.\\n• Support traffic splitting and A/B tests.\\n• Support multi-armed bandit (MAB) tests.\\nModel monitoring\\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production \\nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to \\nbe investigated and updated. Key functionalities in model monitoring include the following:\\n• Measure model efficiency metrics like latency and serving-resource utilization.\\n• Detect data skews, including schema anomalies and data and concept shifts and drifts.\\n• Integrate monitoring with the model evaluation  capability for continuously assessing the effectiveness  \\nperformance of the deployed model when ground truth labels are available. \\nML pipelines\\nThe ML pipelines capability lets you instrument, orchestrate, and automate complex ML training and prediction pipe -\\n\\n14\\nlines in production. ML workflows coordinate different components, where each component performs a specific task \\nin the pipeline. Key functionalities in ML pipelines include the following:\\n• Trigger pipelines on demand, on a schedule, or in response to specified events.\\n• Enable local interactive execution for debugging during ML development.\\n• Integrate with the ML metadata tracking  capability to capture pipeline execution parameters and to produce \\nartifacts.\\n• Provide a set of built-in components for common ML tasks and also allow custom components.\\n• Run on different environments, including local machines and scalable cloud platforms.\\n• Optionally, provide GUI-based tools for designing and building pipelines.\\nModel registry\\nThe model registry capability lets you govern the lifecycle of the ML models in a central repository. This ensures the \\nquality of the production models and enables model discovery. Key functionalities in the model registry include the \\nfollowing:\\n• Register, organize, track, and version your trained and deployed ML models.\\n• Store model metadata and runtime dependencies for deployability.\\n• Maintain model documentation and reporting—for example, using model cards .\\n• Integrate with the model evaluation and deployment capability and track online and offline evaluation metrics \\nfor the models.\\n• Govern the model launching process: review, approve, release, and roll back. These decisions are based on a \\nnumber of offline performance and fairness metrics and on online experimentation results.\\nDataset and feature repository\\nThe dataset and feature repository capability lets you unify the definition and the storage of the ML data assets. \\nHaving a central repository of fresh, high-quality data assets enables shareability, discoverability, and reusability. The \\nrepository also provides data consistency for training and inference. This helps data scientists and ML researchers \\nsave time on data preparation and feature engineering, which typically take up a significant amount of their time. Key \\nfunctionalities in the data and feature repository include the following:\\n\\n15\\n• Enable shareability, discoverability, reusability, and versioning of data assets.\\n• Allow real-time ingestion and low-latency serving for event streaming and online prediction workloads. \\n• Allow high-throughput batch ingestion and serving for extract, transform, load (ETL) processes and model \\ntraining, and for scoring workloads.\\n• Enable feature versioning for point-in-time queries.\\n• Support various data modalities, including tabular data, images, and text.\\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi -\\ntory might contain an entity called customer, which includes features like age group, postal code, and gender. On the \\nother hand, a dataset repository might include a customer churn dataset, which includes features from the customer \\nand product entities, as well as purchase- and web-activity event logs.\\nML metadata and artifact tracking\\nVarious types of ML artifacts are produced in different processes of the MLOps lifecycle, including descriptive \\nstatistics and data schemas, trained models, and evaluation results. ML metadata is the information about these \\nartifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and \\nartifact tracking capability is foundational to all other MLOps capabilities. Such a capability enables reproducibility \\nand debugging of complex ML tasks and pipelines. Key functionalities in ML metadata and artifact tracking include \\nthe following:\\n• Provide traceability and lineage tracking of ML artifacts.\\n• Share and track experimentation and pipeline parameter configurations.\\n• Store, access, investigate, visualize, download, and archive ML artifacts.\\n• Integrate with all other MLOps capabilities.\\nDeep dive of MLOps processes\\nThis section describes each of the core MLOps processes in detail. It describes key tasks and flow of control be -\\ntween tasks, the key artifacts created by the tasks, and the relationship of tasks to other upstream and downstream \\nprocesses. In this section, you learn about concrete details of tasks like running a continuous training pipeline, de -\\nploying a model, and monitoring predictive performance of the model.\\n\\n16\\nMLOps processes take place on an integrated ML platform that has the required development and operations capa -\\nbilities (described later). Infrastructure engineers can provision this type of platform in different environments (like \\ndevelopment, test, staging, and production) using configuration management and infrastructure-as-code (IaC) tools \\nlike Terraform . Each environment is configured with its own set of required compute resources, data access, and \\nsubset of MLOps capability services.\\nML development\\nExperimentation is the core activity in ML development, where your data scientists can rapidly try several ideas for \\ndata preparation and ML modeling. Experimentation starts when the ML use case is well defined, meaning that the \\nfollowing questions have been answered:\\n• What is the task?\\n• How can we measure business impact?\\n• What is the evaluation metric?\\nFigure 5.  The ML development process',\n",
              " '17\\n• What is the relevant data?\\n• What are the training and serving requirements?\\nExperimentation aims to arrive at an effective prototype model for the ML use case at hand. In addition to experimen -\\ntation, data scientists need to formalize their ML training procedures. They do this by implementing an end-to-end \\npipeline, so that the procedures can be operationalized and run in production. Figure 5 shows the process of ML \\ndevelopment. \\nDuring experimentation, data scientists typically perform the following steps:\\n• Data discovery, selection, and exploration.\\n• Data preparation and feature engineering, using interactive data processing tools.\\n• Model prototyping and validation.\\nPerforming these iterative steps can lead data scientists to refining the problem definition. For example, your data \\nscientists or researchers might change the task from regression to classification, or they might opt for another evalu -\\nation metric.\\nThe primary source of development data is the dataset and feature repository . This repository contains curated data \\nassets that are managed on either the entity-features level or the full dataset level.\\nIn general, the key success aspects for this process are experiment tracking , reproducibility, and collaboration. For \\nexample, when your data scientists begin working on an ML use case, it can save them time if they can find previous \\nexperiments that have similar use cases and that reproduce the results of those experiments; data scientists can \\nthen adapt those experiments to the task at hand. In addition, data scientists need to be able to compare various \\nexperiments and to compare different runs of the same experiment so that they understand the factors that lead to \\nchanging the model’s predictive behavior and to performance improvements.\\nTo be able to reproduce an experiment, your data science team needs to track configurations for each experiment, \\nincluding the following:\\n• A pointer to the version of the training code in the version control system.\\n• The model architecture and pretrained modules that were used.\\n• Hyperparameters, including trials of automated hyperparameter tuning and model selection.\\n• Information about training, validation, and testing data splits that were used. \\n• Model evaluation metrics and the validation procedure that was used.\\nIf there is no need to retrain the model on a regular basis, then the produced model at the end of the experimenta -\\ntion is submitted to the model registry. The model is then ready to be reviewed, approved, and deployed to the target\\n\\n18\\nserving environment. In addition, all the relevant metadata and artifacts \\nthat were produced during model development are tracked in the metadata \\ntracking repository.\\nHowever, in most cases, ML models need to be retrained on a regular basis \\nwhen new data is available or when the code changes. In this case, the \\noutput of the ML development process is not the model to be deployed in \\nproduction. Instead, the output is the implementation of the continuous \\ntraining  pipeline to be deployed to the target environment. Whether you use \\ncode-first, low-code, or no-code tools to build continuous training pipelines, \\nthe development artifacts, including source code and configurations, must \\nbe version controlled (for example, using Git-based source control sys -\\ntems). This lets you apply standard software engineering practices to your \\ncode review, code analysis, and automated testing. It also lets you build \\na CI/CD workflow to deploy the continuous training pipeline to the target \\nenvironment.\\nExperimentation activities usually produce novel features and datasets. If \\nthe new data assets are reusable in other ML and analytics use cases, they \\ncan be integrated into the feature and dataset repository through a data \\nengineering pipeline. Therefore, a common output of the experimentation \\nphase is the requirements for upstream data engineering pipelines (see \\nFigure 1).\\nTraining operationalization\\nTraining operationalization is the process of building and testing a repeat -\\nable ML training pipeline and then deploying it to a target execution envi -\\nronment. For MLOps, ML engineers should be able to use configurations to \\ndeploy the ML pipelines. The configurations specify variables like the target \\ndeployment environment (development, test, staging, and so on), the data \\nsources to access during execution in each environment, and the service \\naccount to use for running compute workloads. Figure 6 shows the stages \\nfor an approach for a training pipeline.ML Development\\nTypical assets produced in this \\nprocess include the following:\\n• Notebooks for experimentation  \\nand visualization\\n• Metadata and artifacts of the  \\nexperiments\\n• Data schemas\\n• Query scripts for the training data\\n• Source code and configurations for \\ndata validation and transformation\\n• Source code and configurations for \\ncreating, training, and evaluating \\nmodels\\n• Source code and configurations for \\nthe training-pipeline workflow\\n• Source code for unit tests and  \\nintegration tests\\nCore MLOps capabilities:\\n• Dataset & feature repository\\n• Data processing\\n• Experimentation\\n• Model training\\n• Model registry\\n• ML metadata & artifact repository\\n\\n19\\nA pipeline typically goes through a series of testing and staging environ -\\nments before it is released to production. The number of testing and stag -\\ning environments varies depending on standards that are established in a \\ngiven organization. Most organizations have at least one testing environ -\\nment before production; some have more.\\nThe specifics of the pipeline deployment process depend on the technol -\\nogy that is used to implement the pipeline. With some no-code solutions, \\ndata scientists and ML engineers don’t handle or even see the details.\\nAlternatively, if you use a code-first technology to have more flexibility and \\ncontrol over the ML pipelines, ML engineers can deploy the pipeline using \\nstandard CI/CD processes and tools. This approach is what the diagram \\ndepicts. The diagram shows a standard CI/CD workflow, which consists of \\nthese stages:\\n1. In the CI stage, the source code is unit-tested, and the training pipe -\\nline is built and integration-tested. Any artifacts that are  \\ncreated by the build are stored in an artifact repository.\\nFigure 6.  The training operationalization process\\nTraining  \\nOperationalization\\nTypical assets produced in this \\nprocess include the following:\\n• Training-pipeline executable  \\ncomponents (for example,  \\ncontainer images stored in a  \\ncontainer registry)\\n• Training-pipeline runtime  \\nrepresentation, which references \\nthe components stored in an  \\nartifacts repository\\nCore MLOps capabilities:\\n• ML pipelines\\n\\n20\\n2. In the CD stage, the tested training pipeline artifacts are deployed to a target environment, where the pipeline \\ngoes through end-to-end testing before being released to the production environment. Typically, pipelines are \\ntested in non-production environments on a subset of production data, while the full-scale training is per -\\nformed only in production environments.\\n3. The newly deployed training pipeline is smoke-tested. If the new pipeline fails to produce a deployable model, \\nthe model serving system can fall back to the model that was produced by the current training pipeline.\\nContinuous training\\nThe continuous training process is about orchestrating and automating the execution of training pipelines. How \\nfrequently you retrain your model depends on your use case and on the business value and cost of doing so. For \\nexample, if you are using an ML model that performs well in classifying images, and if there are no changes in the \\nenvironment in which the images are generated and collected, there might be little benefit in retraining the model on \\nnew data every day. On the other hand, if you run a shopping site that has a recommendation system, user behavior \\nchanges continually, and retraining frequently on new data captures new trends and patterns. This investment in \\nretraining can lead to higher click-through rates and therefore potential additional purchases.\\nEach run of the pipeline can be triggered in several ways, including the following:\\n• Scheduled runs based on jobs that you configure.\\n• Event-driven runs, such as when new data becomes available above a certain threshold, or when model de -\\ncay is detected by the continuous monitoring process.\\n• Ad hoc runs based on manual invocation.\\nFigure 7 shows the flow of a canonical pipeline.\\nTypical ML training pipelines have workflows that include to the following:\\n1. Data ingestion. Training data is extracted from the source dataset and feature repository using filtering crite -\\nria such as the date and time of the most recent update.\\n2. Data validation. The extracted training data is validated to make sure that the model is not trained using \\nskewed or corrupted data.\\n3. Data transformation. The data is split, typically into training, evaluation, and testing splits. The data is then \\ntransformed, and the features are engineered as expected by the model.\\n4. Model training and tuning. The ML model is trained and the hyperparameters are tuned using the training and \\nevaluation data splits to produce the best possible model.',\n",
              " '21\\n5. Model evaluation. The model is evaluated against the test data split to assess the performance of the model \\nusing various evaluation metrics on different partitions of the data.\\n6. Model validation. The results of model evaluations are validated to make sure that the model meets the \\nexpected performance criteria.\\n7. Model registration. The validated model is stored in a model registry along with its metadata.\\nAs the diagram shows, the continuous training pipeline runs based on a retraining trigger. When the pipeline starts, it \\nextracts a fresh training dataset from the dataset and feature repository, executes the steps in the ML workflow, and \\nsubmits a trained model to the model registry. All the run information and the artifacts that are produced throughout \\nthe pipeline run are tracked in the metadata and artifact repository.\\nAn orchestrated and automated training pipeline mirrors the steps of the typical data science process that runs in \\nthe ML development phase. However, the automated pipeline has some differences. In an automated pipeline, data \\nvalidation and model validation play a critical role in a way that they don’t during experimentation; these steps are \\ngatekeepers for the overall ML training process. Runs of an automated pipeline are unattended. Therefore, as the \\nruns are executed, the training and evaluation data can evolve to a point where the data processing and training pro -\\ncedures that are implemented by the pipeline are no longer optimal, and possibly are no longer valid.\\nFigure 7.  The continuous training process\\n\\n22\\nThe data validation step can detect when data anomalies start occuring. \\nThese anomalies can include new features, new feature domains, fea -\\ntures that have disappeared, and drastic changes in feature distributions. \\nThe data validation step works by comparing the new training data to the \\nexpected data schema and reference statistics. For details about how data \\nvalidation works, see Analyzing and validating data at scale for machine \\nlearning with TensorFlow Data Validation .\\nThe model validation phase detects anomalies when a lack of improve -\\nment or even a degradation in performance of new model candidates is \\nobserved. The pipeline can apply complex validation logic in this step, \\nincluding several evaluation metrics, sensitivity analysis based on specific \\ninputs, calibration, and fairness indicators.\\nAn important aspect of continuous training, therefore, is tracking. Pipeline \\nruns must track generated metadata and artifacts in a way that enables \\ndebugging, reproducibility, and lineage analysis. Lineage analysis means \\nbeing able to track a trained model back to the dataset (snapshot) that \\nwas used to train that model, and to link all the intermediate artifacts and \\nmetadata. Lineage analysis supports the following:\\n• Retrieve the hyperparameters that are used during training.\\n• Retrieve all evaluations that are performed by the pipeline.\\n• Retrieve processed data snapshots after transformation steps, if \\nfeasible.\\n• Retrieve data summaries such as descriptive statistics, schemas, \\nand feature distributions.\\nWhen a pipeline run has finished and the model has been validated, the \\npipeline can register a model candidate in the model registry. \\nAn automated pipeline can include deployment steps, effectively acting \\nas a unified, end-to-end training and deployment pipeline. In some use \\ncases, where the model is trained and deployed several times per day (for \\nexample, every hour), training and deployment might be implemented and \\nstreamlined into a single pipeline. In these cases, additional validations are \\nbuilt into the pipeline, such as model size validation, serving runtime valida -\\ntion (for dependencies and accelerators), and serving latency evaluation.Continuous  \\nTraining\\nTypical assets produced in this \\nprocess include the following:\\n• A trained and validated model \\nstored in the model registry\\n• Training metadata and artifacts \\nstored in the ML metadata and \\nartifacts repository, including \\npipeline execution parameters, data \\nstatistics, data validation results, \\ntransformed data files, evaluation \\nmetrics, model validation results, \\nand training checkpoints and logs\\nCore MLOps capabilities:\\n• Dataset & feature repository\\n• ML metadata & artifact repository\\n• Data processing\\n• Model training\\n• Model evaluation\\n• ML pipelines\\n• Model registry\\n\\n23\\nHowever, creating a complete pipeline like this might not be practical in all organizations. For example, in some orga -\\nnizations, model training and model deployment are the responsibilities of different teams. Therefore, the scope of \\nmost of the training pipelines ends at registering a trained and validated model, rather than at deploying it for serv -\\ning.\\nModel deployment\\nAfter a model has been trained, validated, and added to the model registry, it is ready for deployment. During the \\nmodel deployment process, the model is packaged, tested, and deployed to a target serving environment. As with the \\ntraining operationalization phase, the model deployment process can involve a number of testing steps and testing \\nenvironments. The model might need to go through a model governance  process before it is allowed to be deployed \\nto a target environment.\\nFigure 8 shows a high-level view of the model deployment process.\\nWhen you use no-code or low-code solutions, the model deployment process is streamlined and abstracted from the \\nperspective of the data scientists and ML engineers. Typically, you point to the entry for the model in the model regis -\\ntry, and the model is deployed automatically using the metadata and the artifacts that are stored for that model.\\nHowever, in other scenarios, you might want more control over the model deployment process, therefore the process \\nFigure 8.  Model deployment progressive delivery workflow\\n\\n24\\nrequires complex CI/CD routines. In that case, the CI/CD system reads the source code of the model serving compo -\\nnent from the source repository and fetches the model from the model registry. The system integrates, builds, tests, \\nand validates the model serving service, and then deploys the service through a progressive delivery process. Figure \\n9 shows this process.\\nIn the CI stage of model deployment, tests might include testing your model interface to see if it accepts the expect -\\ned input format and if it produces the expected output. You might also validate the compatibility of the model with \\nthe target infrastructure, such as checking for required packages and accelerators. During this stage, you might also \\ncheck that the model’s latency is acceptable.\\nIn the CD stage of model deployment, the model undergoes progressive delivery. Canary deployments, blue-green \\ndeployments, and shadow deployments are often used to perform smoke testing, which usually focuses on model \\nservice efficiency like latency and throughput, as well as on service errors. After you verify that the model works \\ntechnically, you test the model’s effectiveness in production by gradually serving it alongside the existing model and \\nrunning online experiments, which refers to testing new functionality in production with live traffic.\\nFigure 9.  A complex CI/CD system for the model deployment process\\n\\n25\\nOnline experimentation is particularly important in the context of ML. \\nDeciding whether a new model candidate should replace the production \\nversion is a more complex and multi-dimensional task compared to deploy -\\ning other software assets.\\nIn the progressive delivery approach, a new model candidate does not \\nimmediately replace the previous version. Instead, after the new model is \\ndeployed to production, it runs in parallel to the previous version. A subset \\nof users is redirected to the new model in stages, according to the online \\nexperiment in place. The outcome of the experiments is the final criterion \\nthat decides whether the model can be fully released and can replace the \\nprevious version. A/B testing  and multi-armed bandit (MAB)  testing are \\ncommon online experimentation techniques that you can use to quantify \\nthe impact of the new model on application-level objectives. Canary and \\nshadow deployment methods can facilitate such online experiments.\\nPrediction serving\\nIn the prediction serving process, after the model is deployed to its tar -\\nget environment, the model service starts to accept prediction requests \\n(serving data) and to serve responses with predictions. Figure 10 shows \\nthe elements of prediction serving.Model  \\nDeployment\\nTypical assets produced in this \\nprocess include the following:\\n• Model serving executable applica -\\ntion (for example, a container image \\nstored in a container registry or a \\nJava package stored in an artifact \\nrepository)\\n• Online experimentation evaluation \\nmetrics stored in ML metadata and \\nartifact repository\\nCore MLOps capabilities:\\n• Model serving\\n• Model registry\\n• Online experimentation\\n• ML metadata & artifact repository\\nFigure 10.  Elements of the prediction serving process',\n",
              " '26\\nThe serving engine can serve predictions to consumers in the following \\nforms:\\n• Online inference in near real time for high-frequency singleton \\nrequests (or mini batches of requests), using interfaces like REST \\nor gRPC.\\n• Streaming inference in near real time, such as through an \\nevent-processing pipeline.\\n• Offline batch inference for bulk data scoring, usually integrated \\nwith extract, transform, load (ETL) processes.\\n• Embedded inference as part of embedded systems or edge devic -\\nes.\\nIn some scenarios of prediction serving, the serving engine might need \\nto look up feature values that are related to the request. For example, you \\nmight have a model that predicts the propensity of a customer to buy a \\nparticular product, given a set of customer and product features. However, \\nthe request includes only the customer and the product identifier. There -\\nfore, the serving engine uses these identifiers to fetch the customer and \\nthe product feature values from a feature repository  and then to feed them \\nto the model to produce a prediction.\\nAn important part of having confidence in ML systems is being able to \\ninterpret the models and provide explanations to their predictions. The \\nexplanations should provide insight into the rationale for the prediction—for \\nexample, by generating feature attributions for a given prediction. Feature \\nattributions  indicate in the form of scores how much each feature contrib -\\nutes to a prediction.\\nThe inference logs and other serving metrics are stored for continuous \\nmonitoring  and analysis.\\nContinuous monitoring\\nContinuous monitoring is the process of monitoring the effectiveness and \\nefficiency of a model in production, which is a crucial area of MLOps. It is \\nessential to regularly and proactively verify that the model performance \\ndoesn’t decay. As the serving data changes over time, its properties start Prediction Serving\\nTypical assets produced in this \\nprocess include the following:\\n• Request-response payloads stored \\nin the serving logs store\\n• Feature attributions of the  \\npredictions\\nCore MLOps capabilities:\\n• Dataset & feature repository\\n• Model serving\\n\\n27\\nto deviate from the properties data that was used for training and evaluating the model. This leads to model effective \\nperformance degradation. In addition, changes or errors in upstream systems that produce the prediction requests \\nmight lead to changes to the properties of the serving data, and consequently produce bad predictions from the \\nmodel.\\nThe monitoring engine uses the inference logs to identify anomalies (skews and outliers), as shown in figure 11.\\nA typical continuous monitoring process consists of the following steps:\\n1. A sample of the request-response payloads is captured and stored in the serving logs store.\\n2. The monitoring engine periodically loads the latest inference logs, generates a schema, and computes statis -\\ntics for the serving data.\\n3. The monitoring engine compares the generated schema to a reference schema to identify schema skews, \\nFigure 11.  The continuous monitoring process\\n\\n28\\nand compares the computed statistics to baseline statistics to \\nidentify distribution skews.\\n4. If the true labels (ground truth) for the serving data are available, \\nthe monitoring engine uses it to evaluate the model predictive \\neffectiveness in hindsight on the serving data.\\n5. If anomalies are identified, or if the model’s performance is decay -\\ning, alerts can be pushed through various channels (for example, \\nemail or chat) to notify the owners to examine the model or to \\ntrigger a new retraining cycle. \\nEffectiveness performance monitoring aims to detect model decay. Model \\ndecay is usually defined in terms of data and concept drift.\\nData drift  describes a growing skew between the dataset that was used to \\ntrain, tune, and evaluate the model and the production data that a model \\nreceives for scoring. Concept drift  is an evolving relationship between the \\ninput predictors and the target feature.\\nData drift can involve two types of skews:\\n• Schema skew  occurs when training data and serving data do not \\nconform to the same schema.\\n• Distribution skew  occurs when the distribution of feature values \\nfor training data is significantly different from the distribution for \\nserving data.\\nIn addition to identifying schema and distribution skews, other techniques \\nfor detecting data and concept drift include novelty and outlier detection, \\nas well as feature attributions change. For more information, see ML model \\nmonitoring reference guides  in the Google Cloud documentation.\\nIn some scenarios, your system is able to store ground truth for your serv -\\ning data. For example, you capture whether a customer bought a product \\nrecommended by your model, or you calculate the actual demand for a \\nparticular product by the end of the week compared to the demand that \\nwas forecasted by the model. You can use this information as true labels \\nto your serving data, and the information can be stored and retrieved from \\nthe dataset and feature repository  for continuous evaluation and for further \\nmodel training cycles.Continuous  \\nMonitoring\\nTypical assets produced in this \\nprocess include the following:\\n• Anomalies detected in serving data \\nduring drift detection\\n• Evaluation metrics produced from \\ncontinuous evaluation\\nCore MLOps capabilities:\\n• Dataset & feature repository\\n• Model monitoring\\n• ML metadata & artifact repository\\n\\n29\\nBesides monitoring model effectiveness, monitoring model serving effi -\\nciency focuses on metrics like the following:\\n• Resource utilization, including CPUs, GPUs, and memory.\\n• Latency, which is a key metric in online and streaming deployments \\nto indicate model service health.\\n• Throughput, which is a key metric in all deployments.\\n• Error rates.\\nMeasuring these metrics is helpful not only in maintaining and improving \\nsystem performance, but also in predicting and managing costs.\\nData and model management\\nAt the heart of the six processes outlined earlier is data and model man -\\nagement. This is a central function for governing ML artifacts in order to \\nsupport auditability, traceability, and compliance, as well as for shareability, \\nreusability, and discoverability of ML assets.\\nDataset and feature management\\nOne of the key challenges of data science and ML is creating, maintaining, \\nand reusing high-quality data for training. Data scientists spend a signifi -\\ncant amount of their ML development time on exploratory data analysis, \\ndata preparation, and data transformation. However, other teams might \\nhave prepared the same datasets for similar use cases but have no means \\nfor sharing and reusing them. This situation can lead not only to wasted \\ntime re-creating the datasets, but to inconsistent definitions and instances \\nof the same data entities.\\nIn addition, during prediction serving, a common challenge is discrepancies \\nbetween serving data and training data. This is called training-serving skew , \\nand can occur because the data is extracted from different sources in dif -\\nferent forms during training and serving. Training-serving skew impacts the \\nperformance of the models in production.\\nDataset and feature management helps mitigate such issues by providing Data & Model  \\nManagement \\nCore MLOps capabilities:\\n• Dataset & feature repository\\n• Model registry\\n• ML metadata & artifact repository\\n\\n30\\na unified repository for ML features and datasets. Figure 12 shows how the feature and dataset repository provides \\nthe same set of data entities for multiple uses in the MLOps environment.\\nAs the diagram shows, the features and datasets are created, discovered, and reused in different experiments. Batch \\nserving of the data is used for experimentation, continuous training, and batch prediction, while online serving of the \\ndata is used for real-time prediction use cases.\\nFeature management\\nFeatures are attributes of business entities that are cleansed and prepared based on standard business rules—ag -\\ngregations, derivations, flags, and so on. Examples of entities include product, customer, location, and promotion. \\nYou can manage your data entities in a centralized repository to standardize their definition, storage, and access for \\ntraining and serving. A feature repository helps data scientists and researchers do the following:\\n• Discover and reuse available feature sets for their entities instead of re-creating the entities in order to create \\nFigure 12.  Using the dataset and feature repository to provide entities for multiple uses',\n",
              " '31\\ntheir own datasets.\\n• Establish a central definition of features.\\n• Avoid training-serving skew by using the feature repository as the data source for experimentation, continu -\\nous training, and online serving.\\n• Serve up-to-date feature values from the feature repository.\\n• Provide a way of defining and sharing new entities and features.\\n• Improve collaboration between data science and research teams by sharing features.\\nIn batch ETL systems, the training pipeline can retrieve the features as a batch for the training task. For online serv -\\ning, the serving engine can fetch the feature values that are related to the requested entity. Updates to the feature \\nrepository can be ingested from the batch ETL or streaming systems. In addition to those updates, the monitoring \\nservice can update statistics and metrics for these features.\\nDataset management\\nFeatures can be used in many datasets for multiple ML tasks and use cases, while a dataset is used for a particular \\nML task or use case. More precisely, feature repositories typically don’t include labeled data instances (instances \\nwith predictable targets). Instead, they include reusable feature values of various entities. The features of different \\nentities can be combined and joined with other transactional data that contains labels in order to create a dataset.\\nFor example, the feature repository might contain a customer entity that includes features that describe customer \\ndemographics, purchase behavior, social media interactions, sentiment scores, third-party financial flags, and so on. \\nThe customer entity can be used in several tasks, such as churn prediction, click-through rate prediction, customer \\nlifetime value estimation, customer segmentation, and recommendations. Each task has its own dataset that con -\\ntains the customer features and other features from the entities that are relevant to the task. In addition, in case of \\nsupervised learning tasks, each dataset has its own labels.\\nDataset management helps with the following:\\n• Maintaining scripts for creating datasets and splits so that datasets can be created in different environments \\n(development, test, production, and so on).\\n• Maintaining a single dataset definition and realization within the team to use in various model implementa -\\ntions and hyperparameters. This dataset includes splits (training, evaluation, test, and so on) and filtering \\nconditions.\\n• Maintaining metadata and annotation that can be useful for team members who are collaborating on the \\nsame dataset and task.\\n• Providing reproducibility and lineage tracking.\\n\\n32\\nModel management\\nAs organizations add to the number of models in production at scale, it becomes difficult to keep track of all of them \\nmanually. Organizations need controls in order to manage risk and implement ML models responsibility, as well as to \\nmaintain compliance with regulations. \\nTo help with this task, organizations need to establish robust model management. Model management is a \\ncross-cutting process that is at the center of MLOps. It entails both ML metadata tracking and model governance. \\nHaving model management across the ML lifecycle helps ensure the following:\\n• The data that is being collected and used for model training and evaluation is accurate, unbiased, and used \\nappropriately without any data privacy violations.\\n• The models are evaluated and validated against effectiveness quality measures and fairness indicators, so \\nthat they are fit for deployment in production.\\n• The models are interpretable, and their outcomes are explainable (if needed).\\n• The performance of deployed models is monitored using continuous evaluation and the models’ perfor -\\nmance metrics are tracked and reported. \\n• Potential issues in model training or prediction serving are traceable, debuggable, and reproducible.\\nML metadata tracking\\nFigure 13 . Metadata tracking\\n\\n33\\nML metadata tracking is generally integrated with different MLOps processes. The artifacts produced by the other \\nprocesses are usually automatically stored in an ML artifact repository, along with the information about the process \\nexecutions. ML metadata that is captured can include pipeline run ID, trigger, process type, step, start and end date -\\ntime, status, environment configurations, and input parameter values. Examples of artifacts that are stored include \\nprocessed data splits, schemas, statistics, hyperparameters, models, and evaluation metrics or custom artifacts. \\nFigure 13 shows metadata tracking.\\nML metadata tracking lets data scientists and ML engineers track experimentation parameters and pipeline config -\\nurations for reproducibility and for tracing lineage. In addition, ML metadata tracking lets users search, discover, and \\nexport existing ML models and artifacts. \\nData scientists and ML engineers can use ML metadata tracking to add and update annotations to the tracked ML \\nexperiments and runs. This facilitates discoverability. Moreover, ML metadata tracking provides the tools for analyz -\\ning, comparing, and visualizing the metadata and artifacts of different experiments and ML pipeline runs. This helps \\ndata scientists and ML engineers understand the behavior of the pipelines and to debug ML issues.\\nModel governance\\nModel governance is about registering, reviewing, validating, and approving models for deployment. Depending on \\nthe organization, on the regulatory requirements of the model, and on the particular use case, the process of model \\ngovernance can differ. The process can be automated, semi-automated, or fully automated (with multiple release \\ncriteria in all cases) to determine whether ML models are ready to go to production. In addition, model governance \\nshould support reporting on the performance of deployed models.\\nFigure 14.  Tasks involved in model governance\\n\\n34\\nFigure 14 shows the tasks that are involved in model governance.\\nModel governance can use information in the ML metadata and the model registry to do the following tasks:\\n• Store:  Add or update model properties and track model versions and property changes. The model registry can \\nstore many model versions from the experimentation and continuous training phases, which lets data scientists \\neasily reproduce significant models.\\n• Evaluate:  Compare a new challenger model to the champion model by looking not only at evaluation metrics \\n(accuracy, precision, recall, specificity, and so on) but also at business KPIs that are collected through online ex -\\nperimentation. Additionally, model owners need to be able to understand and explain the model predictions—for \\nexample, by using feature attribution methods. This ensures the quality of the model that is deployed in produc -\\ntion.\\n• Check:  Review, request changes, and approve the model to help control for risks, such as business, financial, \\nlegal, security, privacy, reputational, and ethical concerns.\\n• Release:  Invoke the model deployment process to go live. This controls the type of the model release (for exam -\\nple, canary or blue-green) and the traffic split that is directed to it.\\n• Report:  Aggregate, visualize, and highlight model performance metrics that are collected from the continuous \\nevaluation process. This ensures the quality of the model in production.\\nExplainability is particularly important in the case of decision automation. The governance process should provide \\nto risk managers and auditors a clear view of lineage and accountability. The process should also provide them the \\nability to review decisions in accordance with an organization’s ethical and legal responsibilities.\\nPutting it all together\\nDelivering business value through ML is not only about building the best ML model for the use case at hand. Deliver -\\ning this value is also about building an integrated ML system that operates continuously to adapt to changes in the \\ndynamics of the business environment. Such an ML system involves collecting, processing, and managing ML data -\\nsets and features; training, and evaluating models at scale; serving the model for predictions; monitoring the model \\nperformance in production; and tracking model metadata and artifacts.\\nIn this document, we discuss the core capabilities for building and operating ML systems, and we describe a com -\\nprehensive MLOps process to streamline the workflow from development to production. This can help organizations \\nreduce time to market while increasing the reliability, performance, scalability, and security of their ML systems. \\nFigure 15 provides a summary of the end-to-end MLOps process.\\n\\n35\\nFigure 15.  End-to-end MLOps workflow\\n\\n36\\nAdditional resources\\nFor more information about how to get started with MLOps on Google Cloud, see the following books, guides, cours -\\nes, articles, and videos:\\n• Best Practices for Implementing Machine Learning on Google Cloud\\n• Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, \\nand MLOps\\n• An introduction to MLOps on Google Cloud\\n• MLOps: Continuous delivery and automation pipelines in machine learning\\n• Architecture for MLOps using TFX, Kubeflow Pipelines, and Cloud Build\\n• Setting up an MLOps environment on Google Cloud\\n• MLOps (Machine Learning Operations) Fundamentals on Coursera']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymQMy6fPzyo4",
        "outputId": "dfab1cc2-d1cc-49ad-8233-ba1ccdbf3056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleGenerativeAIEmbeddings(model='models/embedding-001', task_type=None, google_api_key=None, credentials=None, client_options=None, transport=None, request_options=None)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = Chroma.from_texts(texts, embeddings).as_retriever()\n",
        "vector_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgBJfgX-z6Rk",
        "outputId": "88d7700f-f351-4a98-f822-654aa41b13d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7b9de8ba00a0>)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform semantic search.\n",
        "docs = vector_index.get_relevant_documents(question)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd9XdaSU0B3G",
        "outputId": "a7aedfba-f1db-426a-a057-c99e99f4c4e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='26\\nThe serving engine can serve predictions to consumers in the following \\nforms:\\n• Online inference in near real time for high-frequency singleton \\nrequests (or mini batches of requests), using interfaces like REST \\nor gRPC.\\n• Streaming inference in near real time, such as through an \\nevent-processing pipeline.\\n• Offline batch inference for bulk data scoring, usually integrated \\nwith extract, transform, load (ETL) processes.\\n• Embedded inference as part of embedded systems or edge devic -\\nes.\\nIn some scenarios of prediction serving, the serving engine might need \\nto look up feature values that are related to the request. For example, you \\nmight have a model that predicts the propensity of a customer to buy a \\nparticular product, given a set of customer and product features. However, \\nthe request includes only the customer and the product identifier. There -\\nfore, the serving engine uses these identifiers to fetch the customer and \\nthe product feature values from a feature repository  and then to feed them \\nto the model to produce a prediction.\\nAn important part of having confidence in ML systems is being able to \\ninterpret the models and provide explanations to their predictions. The \\nexplanations should provide insight into the rationale for the prediction—for \\nexample, by generating feature attributions for a given prediction. Feature \\nattributions  indicate in the form of scores how much each feature contrib -\\nutes to a prediction.\\nThe inference logs and other serving metrics are stored for continuous \\nmonitoring  and analysis.\\nContinuous monitoring\\nContinuous monitoring is the process of monitoring the effectiveness and \\nefficiency of a model in production, which is a crucial area of MLOps. It is \\nessential to regularly and proactively verify that the model performance \\ndoesn’t decay. As the serving data changes over time, its properties start Prediction Serving\\nTypical assets produced in this \\nprocess include the following:\\n• Request-response payloads stored \\nin the serving logs store\\n• Feature attributions of the  \\npredictions\\nCore MLOps capabilities:\\n• Dataset & feature repository\\n• Model serving\\n\\n27\\nto deviate from the properties data that was used for training and evaluating the model. This leads to model effective \\nperformance degradation. In addition, changes or errors in upstream systems that produce the prediction requests \\nmight lead to changes to the properties of the serving data, and consequently produce bad predictions from the \\nmodel.\\nThe monitoring engine uses the inference logs to identify anomalies (skews and outliers), as shown in figure 11.\\nA typical continuous monitoring process consists of the following steps:\\n1. A sample of the request-response payloads is captured and stored in the serving logs store.\\n2. The monitoring engine periodically loads the latest inference logs, generates a schema, and computes statis -\\ntics for the serving data.\\n3. The monitoring engine compares the generated schema to a reference schema to identify schema skews, \\nFigure 11.  The continuous monitoring process\\n\\n28\\nand compares the computed statistics to baseline statistics to \\nidentify distribution skews.\\n4. If the true labels (ground truth) for the serving data are available, \\nthe monitoring engine uses it to evaluate the model predictive \\neffectiveness in hindsight on the serving data.\\n5. If anomalies are identified, or if the model’s performance is decay -\\ning, alerts can be pushed through various channels (for example, \\nemail or chat) to notify the owners to examine the model or to \\ntrigger a new retraining cycle. \\nEffectiveness performance monitoring aims to detect model decay. Model \\ndecay is usually defined in terms of data and concept drift.\\nData drift  describes a growing skew between the dataset that was used to \\ntrain, tune, and evaluate the model and the production data that a model \\nreceives for scoring. Concept drift  is an evolving relationship between the \\ninput predictors and the target feature.\\nData drift can involve two types of skews:\\n• Schema skew  occurs when training data and serving data do not \\nconform to the same schema.\\n• Distribution skew  occurs when the distribution of feature values \\nfor training data is significantly different from the distribution for \\nserving data.\\nIn addition to identifying schema and distribution skews, other techniques \\nfor detecting data and concept drift include novelty and outlier detection, \\nas well as feature attributions change. For more information, see ML model \\nmonitoring reference guides  in the Google Cloud documentation.\\nIn some scenarios, your system is able to store ground truth for your serv -\\ning data. For example, you capture whether a customer bought a product \\nrecommended by your model, or you calculate the actual demand for a \\nparticular product by the end of the week compared to the demand that \\nwas forecasted by the model. You can use this information as true labels \\nto your serving data, and the information can be stored and retrieved from \\nthe dataset and feature repository  for continuous evaluation and for further \\nmodel training cycles.Continuous  \\nMonitoring\\nTypical assets produced in this \\nprocess include the following:\\n• Anomalies detected in serving data \\nduring drift detection\\n• Evaluation metrics produced from \\ncontinuous evaluation\\nCore MLOps capabilities:\\n• Dataset & feature repository\\n• Model monitoring\\n• ML metadata & artifact repository\\n\\n29\\nBesides monitoring model effectiveness, monitoring model serving effi -\\nciency focuses on metrics like the following:\\n• Resource utilization, including CPUs, GPUs, and memory.\\n• Latency, which is a key metric in online and streaming deployments \\nto indicate model service health.\\n• Throughput, which is a key metric in all deployments.\\n• Error rates.\\nMeasuring these metrics is helpful not only in maintaining and improving \\nsystem performance, but also in predicting and managing costs.\\nData and model management\\nAt the heart of the six processes outlined earlier is data and model man -\\nagement. This is a central function for governing ML artifacts in order to \\nsupport auditability, traceability, and compliance, as well as for shareability, \\nreusability, and discoverability of ML assets.\\nDataset and feature management\\nOne of the key challenges of data science and ML is creating, maintaining, \\nand reusing high-quality data for training. Data scientists spend a signifi -\\ncant amount of their ML development time on exploratory data analysis, \\ndata preparation, and data transformation. However, other teams might \\nhave prepared the same datasets for similar use cases but have no means \\nfor sharing and reusing them. This situation can lead not only to wasted \\ntime re-creating the datasets, but to inconsistent definitions and instances \\nof the same data entities.\\nIn addition, during prediction serving, a common challenge is discrepancies \\nbetween serving data and training data. This is called training-serving skew , \\nand can occur because the data is extracted from different sources in dif -\\nferent forms during training and serving. Training-serving skew impacts the \\nperformance of the models in production.\\nDataset and feature management helps mitigate such issues by providing Data & Model  \\nManagement \\nCore MLOps capabilities:\\n• Dataset & feature repository\\n• Model registry\\n• ML metadata & artifact repository\\n\\n30\\na unified repository for ML features and datasets. Figure 12 shows how the feature and dataset repository provides \\nthe same set of data entities for multiple uses in the MLOps environment.\\nAs the diagram shows, the features and datasets are created, discovered, and reused in different experiments. Batch \\nserving of the data is used for experimentation, continuous training, and batch prediction, while online serving of the \\ndata is used for real-time prediction use cases.\\nFeature management\\nFeatures are attributes of business entities that are cleansed and prepared based on standard business rules—ag -\\ngregations, derivations, flags, and so on. Examples of entities include product, customer, location, and promotion. \\nYou can manage your data entities in a centralized repository to standardize their definition, storage, and access for \\ntraining and serving. A feature repository helps data scientists and researchers do the following:\\n• Discover and reuse available feature sets for their entities instead of re-creating the entities in order to create \\nFigure 12.  Using the dataset and feature repository to provide entities for multiple uses'),\n",
              " Document(page_content='12\\nModel training should be able to scale with the size of both the models and the datasets that are used for training. \\nKey functionalities in model training include the following:\\n• Support common ML frameworks and support custom runtime environments.\\n• Support large-scale distributed training with different strategies for multiple GPUs and multiple workers.\\n• Enable on-demand use of ML accelerators.\\n• Allow efficient hyperparameter tuning and target optimization at scale.\\n• Ideally, provide built-in automated ML (AutoML) functionality, including automated feature selection and engi -\\nneering as well as automated model architecture search and selection.\\nModel evaluation\\nThe model evaluation capability lets you assess the effectiveness of your model, interactively during experimentation \\nand automatically in production. Key functionalities in model evaluation include the following:\\n• Perform batch scoring of your models on evaluation datasets at scale.\\n• Compute pre-defined or custom evaluation metrics for your model on different slices of the data.\\n• Track trained-model predictive performance across different continuous-training executions.\\n• Visualize and compare performances of different models.\\n• Provide tools for what-if analysis and for identifying bias and fairness issues.\\n• Enable model behavior interpretation using various explainable AI techniques.\\nModel serving\\nThe model serving capability lets you deploy and serve your models in production environments. Key functionalities \\nin model serving include the following:\\n• Provide support for low-latency, near-real-time (online) prediction and high-throughput batch (offline)  \\nprediction.\\n• Provide built-in support for common ML serving frameworks (for example, TensorFlow Serving , TorchServe , \\nNvidia Triton , and others for Scikit-learn  and XGBoost  models) and for custom runtime environments.\\n• Enable composite prediction routines, where multiple models are invoked hierarchically or simultaneously \\nbefore the results are aggregated, in addition to any required pre- or post-processing routines.\\n• Allow efficient use of ML inference accelerators with autoscaling to match spiky workloads and to balance\\n\\n13\\ncost with latency.\\n• Support model explainability using techniques like feature attributions for a given model prediction.\\n• Support logging of prediction serving requests and responses for analysis. \\nOnline experimentation\\nThe online experimentation capability lets you understand how newly trained models perform in production settings \\ncompared to the current models (if any) before you release the new model to production. For example, using a small \\nsubset of the serving population, you use online experimentation to understand the impact that a new recommen -\\ndation system has on click-throughs and on conversation rates. The results of online experimentation should be \\nintegrated with the model registry  capability to facilitate the decision about releasing the model to production. Online \\nexperimentation enhances the reliability of your ML releases by helping you decide to discard ill-performing models \\nand to promote well-performing ones. Key functionalities in online experimentation include the following:\\n• Support canary and shadow deployments.\\n• Support traffic splitting and A/B tests.\\n• Support multi-armed bandit (MAB) tests.\\nModel monitoring\\nThe model monitoring capability lets you track the efficiency and effectiveness of the deployed models in production \\nto ensure predictive quality and business continuity. This capability informs you if your models are stale and need to \\nbe investigated and updated. Key functionalities in model monitoring include the following:\\n• Measure model efficiency metrics like latency and serving-resource utilization.\\n• Detect data skews, including schema anomalies and data and concept shifts and drifts.\\n• Integrate monitoring with the model evaluation  capability for continuously assessing the effectiveness  \\nperformance of the deployed model when ground truth labels are available. \\nML pipelines\\nThe ML pipelines capability lets you instrument, orchestrate, and automate complex ML training and prediction pipe -\\n\\n14\\nlines in production. ML workflows coordinate different components, where each component performs a specific task \\nin the pipeline. Key functionalities in ML pipelines include the following:\\n• Trigger pipelines on demand, on a schedule, or in response to specified events.\\n• Enable local interactive execution for debugging during ML development.\\n• Integrate with the ML metadata tracking  capability to capture pipeline execution parameters and to produce \\nartifacts.\\n• Provide a set of built-in components for common ML tasks and also allow custom components.\\n• Run on different environments, including local machines and scalable cloud platforms.\\n• Optionally, provide GUI-based tools for designing and building pipelines.\\nModel registry\\nThe model registry capability lets you govern the lifecycle of the ML models in a central repository. This ensures the \\nquality of the production models and enables model discovery. Key functionalities in the model registry include the \\nfollowing:\\n• Register, organize, track, and version your trained and deployed ML models.\\n• Store model metadata and runtime dependencies for deployability.\\n• Maintain model documentation and reporting—for example, using model cards .\\n• Integrate with the model evaluation and deployment capability and track online and offline evaluation metrics \\nfor the models.\\n• Govern the model launching process: review, approve, release, and roll back. These decisions are based on a \\nnumber of offline performance and fairness metrics and on online experimentation results.\\nDataset and feature repository\\nThe dataset and feature repository capability lets you unify the definition and the storage of the ML data assets. \\nHaving a central repository of fresh, high-quality data assets enables shareability, discoverability, and reusability. The \\nrepository also provides data consistency for training and inference. This helps data scientists and ML researchers \\nsave time on data preparation and feature engineering, which typically take up a significant amount of their time. Key \\nfunctionalities in the data and feature repository include the following:\\n\\n15\\n• Enable shareability, discoverability, reusability, and versioning of data assets.\\n• Allow real-time ingestion and low-latency serving for event streaming and online prediction workloads. \\n• Allow high-throughput batch ingestion and serving for extract, transform, load (ETL) processes and model \\ntraining, and for scoring workloads.\\n• Enable feature versioning for point-in-time queries.\\n• Support various data modalities, including tabular data, images, and text.\\nML data assets can be managed at the entity features level or at the full dataset level. For example, a feature reposi -\\ntory might contain an entity called customer, which includes features like age group, postal code, and gender. On the \\nother hand, a dataset repository might include a customer churn dataset, which includes features from the customer \\nand product entities, as well as purchase- and web-activity event logs.\\nML metadata and artifact tracking\\nVarious types of ML artifacts are produced in different processes of the MLOps lifecycle, including descriptive \\nstatistics and data schemas, trained models, and evaluation results. ML metadata is the information about these \\nartifacts, including their location, types, properties, and associations to experiments and runs. The ML metadata and \\nartifact tracking capability is foundational to all other MLOps capabilities. Such a capability enables reproducibility \\nand debugging of complex ML tasks and pipelines. Key functionalities in ML metadata and artifact tracking include \\nthe following:\\n• Provide traceability and lineage tracking of ML artifacts.\\n• Share and track experimentation and pipeline parameter configurations.\\n• Store, access, investigate, visualize, download, and archive ML artifacts.\\n• Integrate with all other MLOps capabilities.\\nDeep dive of MLOps processes\\nThis section describes each of the core MLOps processes in detail. It describes key tasks and flow of control be -\\ntween tasks, the key artifacts created by the tasks, and the relationship of tasks to other upstream and downstream \\nprocesses. In this section, you learn about concrete details of tasks like running a continuous training pipeline, de -\\nploying a model, and monitoring predictive performance of the model.\\n\\n16\\nMLOps processes take place on an integrated ML platform that has the required development and operations capa -\\nbilities (described later). Infrastructure engineers can provision this type of platform in different environments (like \\ndevelopment, test, staging, and production) using configuration management and infrastructure-as-code (IaC) tools \\nlike Terraform . Each environment is configured with its own set of required compute resources, data access, and \\nsubset of MLOps capability services.\\nML development\\nExperimentation is the core activity in ML development, where your data scientists can rapidly try several ideas for \\ndata preparation and ML modeling. Experimentation starts when the ML use case is well defined, meaning that the \\nfollowing questions have been answered:\\n• What is the task?\\n• How can we measure business impact?\\n• What is the evaluation metric?\\nFigure 5.  The ML development process'),\n",
              " Document(page_content='31\\ntheir own datasets.\\n• Establish a central definition of features.\\n• Avoid training-serving skew by using the feature repository as the data source for experimentation, continu -\\nous training, and online serving.\\n• Serve up-to-date feature values from the feature repository.\\n• Provide a way of defining and sharing new entities and features.\\n• Improve collaboration between data science and research teams by sharing features.\\nIn batch ETL systems, the training pipeline can retrieve the features as a batch for the training task. For online serv -\\ning, the serving engine can fetch the feature values that are related to the requested entity. Updates to the feature \\nrepository can be ingested from the batch ETL or streaming systems. In addition to those updates, the monitoring \\nservice can update statistics and metrics for these features.\\nDataset management\\nFeatures can be used in many datasets for multiple ML tasks and use cases, while a dataset is used for a particular \\nML task or use case. More precisely, feature repositories typically don’t include labeled data instances (instances \\nwith predictable targets). Instead, they include reusable feature values of various entities. The features of different \\nentities can be combined and joined with other transactional data that contains labels in order to create a dataset.\\nFor example, the feature repository might contain a customer entity that includes features that describe customer \\ndemographics, purchase behavior, social media interactions, sentiment scores, third-party financial flags, and so on. \\nThe customer entity can be used in several tasks, such as churn prediction, click-through rate prediction, customer \\nlifetime value estimation, customer segmentation, and recommendations. Each task has its own dataset that con -\\ntains the customer features and other features from the entities that are relevant to the task. In addition, in case of \\nsupervised learning tasks, each dataset has its own labels.\\nDataset management helps with the following:\\n• Maintaining scripts for creating datasets and splits so that datasets can be created in different environments \\n(development, test, production, and so on).\\n• Maintaining a single dataset definition and realization within the team to use in various model implementa -\\ntions and hyperparameters. This dataset includes splits (training, evaluation, test, and so on) and filtering \\nconditions.\\n• Maintaining metadata and annotation that can be useful for team members who are collaborating on the \\nsame dataset and task.\\n• Providing reproducibility and lineage tracking.\\n\\n32\\nModel management\\nAs organizations add to the number of models in production at scale, it becomes difficult to keep track of all of them \\nmanually. Organizations need controls in order to manage risk and implement ML models responsibility, as well as to \\nmaintain compliance with regulations. \\nTo help with this task, organizations need to establish robust model management. Model management is a \\ncross-cutting process that is at the center of MLOps. It entails both ML metadata tracking and model governance. \\nHaving model management across the ML lifecycle helps ensure the following:\\n• The data that is being collected and used for model training and evaluation is accurate, unbiased, and used \\nappropriately without any data privacy violations.\\n• The models are evaluated and validated against effectiveness quality measures and fairness indicators, so \\nthat they are fit for deployment in production.\\n• The models are interpretable, and their outcomes are explainable (if needed).\\n• The performance of deployed models is monitored using continuous evaluation and the models’ perfor -\\nmance metrics are tracked and reported. \\n• Potential issues in model training or prediction serving are traceable, debuggable, and reproducible.\\nML metadata tracking\\nFigure 13 . Metadata tracking\\n\\n33\\nML metadata tracking is generally integrated with different MLOps processes. The artifacts produced by the other \\nprocesses are usually automatically stored in an ML artifact repository, along with the information about the process \\nexecutions. ML metadata that is captured can include pipeline run ID, trigger, process type, step, start and end date -\\ntime, status, environment configurations, and input parameter values. Examples of artifacts that are stored include \\nprocessed data splits, schemas, statistics, hyperparameters, models, and evaluation metrics or custom artifacts. \\nFigure 13 shows metadata tracking.\\nML metadata tracking lets data scientists and ML engineers track experimentation parameters and pipeline config -\\nurations for reproducibility and for tracing lineage. In addition, ML metadata tracking lets users search, discover, and \\nexport existing ML models and artifacts. \\nData scientists and ML engineers can use ML metadata tracking to add and update annotations to the tracked ML \\nexperiments and runs. This facilitates discoverability. Moreover, ML metadata tracking provides the tools for analyz -\\ning, comparing, and visualizing the metadata and artifacts of different experiments and ML pipeline runs. This helps \\ndata scientists and ML engineers understand the behavior of the pipelines and to debug ML issues.\\nModel governance\\nModel governance is about registering, reviewing, validating, and approving models for deployment. Depending on \\nthe organization, on the regulatory requirements of the model, and on the particular use case, the process of model \\ngovernance can differ. The process can be automated, semi-automated, or fully automated (with multiple release \\ncriteria in all cases) to determine whether ML models are ready to go to production. In addition, model governance \\nshould support reporting on the performance of deployed models.\\nFigure 14.  Tasks involved in model governance\\n\\n34\\nFigure 14 shows the tasks that are involved in model governance.\\nModel governance can use information in the ML metadata and the model registry to do the following tasks:\\n• Store:  Add or update model properties and track model versions and property changes. The model registry can \\nstore many model versions from the experimentation and continuous training phases, which lets data scientists \\neasily reproduce significant models.\\n• Evaluate:  Compare a new challenger model to the champion model by looking not only at evaluation metrics \\n(accuracy, precision, recall, specificity, and so on) but also at business KPIs that are collected through online ex -\\nperimentation. Additionally, model owners need to be able to understand and explain the model predictions—for \\nexample, by using feature attribution methods. This ensures the quality of the model that is deployed in produc -\\ntion.\\n• Check:  Review, request changes, and approve the model to help control for risks, such as business, financial, \\nlegal, security, privacy, reputational, and ethical concerns.\\n• Release:  Invoke the model deployment process to go live. This controls the type of the model release (for exam -\\nple, canary or blue-green) and the traffic split that is directed to it.\\n• Report:  Aggregate, visualize, and highlight model performance metrics that are collected from the continuous \\nevaluation process. This ensures the quality of the model in production.\\nExplainability is particularly important in the case of decision automation. The governance process should provide \\nto risk managers and auditors a clear view of lineage and accountability. The process should also provide them the \\nability to review decisions in accordance with an organization’s ethical and legal responsibilities.\\nPutting it all together\\nDelivering business value through ML is not only about building the best ML model for the use case at hand. Deliver -\\ning this value is also about building an integrated ML system that operates continuously to adapt to changes in the \\ndynamics of the business environment. Such an ML system involves collecting, processing, and managing ML data -\\nsets and features; training, and evaluating models at scale; serving the model for predictions; monitoring the model \\nperformance in production; and tracking model metadata and artifacts.\\nIn this document, we discuss the core capabilities for building and operating ML systems, and we describe a com -\\nprehensive MLOps process to streamline the workflow from development to production. This can help organizations \\nreduce time to market while increasing the reliability, performance, scalability, and security of their ML systems. \\nFigure 15 provides a summary of the end-to-end MLOps process.\\n\\n35\\nFigure 15.  End-to-end MLOps workflow\\n\\n36\\nAdditional resources\\nFor more information about how to get started with MLOps on Google Cloud, see the following books, guides, cours -\\nes, articles, and videos:\\n• Best Practices for Implementing Machine Learning on Google Cloud\\n• Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, \\nand MLOps\\n• An introduction to MLOps on Google Cloud\\n• MLOps: Continuous delivery and automation pipelines in machine learning\\n• Architecture for MLOps using TFX, Kubeflow Pipelines, and Cloud Build\\n• Setting up an MLOps environment on Google Cloud\\n• MLOps (Machine Learning Operations) Fundamentals on Coursera'),\n",
              " Document(page_content='6\\nquired for a robust MLOps implementation.\\nBuilding an ML-enabled system\\nBuilding an ML-enabled system is a multifaceted undertaking that combines data engineering, ML engineering, and \\napplication engineering tasks, as shown in figure 1.\\nData engineering involves ingesting, integrating, curating, and refining data to facilitate a broad spectrum of opera -\\ntional tasks, data analytics tasks, and ML tasks. Data engineering can be crucial to the success of the analytics and \\nML initiatives. If an organization does not have robust data engineering processes and technologies, it might not be \\nset up for success with downstream business intelligence, advanced analytics, or ML projects.\\nML models are built and deployed in production using curated data that is usually created by the data engineering \\nteam. The models do not operate in silos; they are components of, and support, a large range of application systems, \\nsuch as business intelligence systems, line of business applications, process control systems, and embedded sys -\\ntems. Integrating an ML model into an application is a critical task that involves making sure first that the deployed \\nmodel is used effectively by the applications, and then monitoring model performance. In addition to this, you should \\nalso collect and monitor relevant business KPIs (for example, click-through rate, revenue uplift, and user experience). \\nThis information helps you understand the impact of the ML model on the business and adapt accordingly.\\nFigure 1.  The relationship of data engineering, ML engineering, and app engineering\\n\\n7\\nThe MLOps lifecycle\\nThe MLOps lifecycle encompasses seven integrated and iterative processes, as shown in figure 2.\\nThe processes can consist of the following:\\n• ML development  concerns experimenting and developing a robust and reproducible model training proce -\\ndure (training pipeline code), which consists of multiple tasks from data preparation and transformation to \\nmodel training and evaluation.\\n• Training operationalization  concerns automating the process of packaging, testing, and deploying repeat -\\nable and reliable training pipelines.\\n• Continuous training  concerns repeatedly executing the training pipeline in response to new data or to code \\nchanges, or on a schedule, potentially with new training settings.\\n• Model deployment  concerns packaging, testing, and deploying a model to a serving environment for online \\nexperimentation and production serving.\\nFigure 2 . The MLOps lifecycle\\n\\n8\\n• Prediction serving  is about serving the model that is deployed in production for inference.\\n• Continuous monitoring  is about monitoring the effectiveness and efficiency of a deployed model.\\n• Data and model management  is a central, cross-cutting function for governing ML artifacts to support audit -\\nability, traceability, and compliance. Data and model management can also promote shareability, reusability, \\nand discoverability of ML assets.\\nMLOps: An end-to-end workflow\\nFigure 3 shows a simplified but canonical flow for how the MLOps processes interact with each other, focusing on \\nhigh-level flow of control and on key inputs and outputs.\\nThis is not a waterfall workflow that has to sequentially pass through all the processes. The processes can be \\nskipped, or the flow can repeat a given phase or a subsequence of the processes. The diagram shows the following \\nflow:\\n1. The core activity during this ML development phase is experimentation. As data scientists and ML research -\\ners prototype model architectures and training routines, they create labeled datasets, and they use features \\nand other reusable ML artifacts that are governed through the data and model management process. The \\nFigure 3.  The MLOps process\\n\\n9\\nprimary output of this process is a formalized training procedure, which includes data preprocessing, model \\narchitecture, and model training settings. \\n2. If the ML system requires continuous training (repeated retraining of the model), the training procedure is \\noperationalized as a training pipeline. This requires a CI/CD routine to build, test, and deploy the pipeline to \\nthe target execution environment.\\n3. The continuous training pipeline is executed repeatedly based on retraining triggers, and it produces a model \\nas output. The model is retrained as new data becomes available, or if model performance decay is detected. \\nOther training artifacts and metadata that are produced by a training pipeline are also tracked. If the pipeline \\nproduces a successful model candidate, that candidate is then tracked by the model management process \\nas a registered model. \\n4. The registered model is annotated, reviewed, and approved for release and is then deployed to a production \\nenvironment. This process might be relatively opaque if you are using a no-code solution, or it can involve \\nbuilding a custom CI/CD pipeline for progressive delivery. \\n5. The deployed model serves predictions using the deployment pattern that you have specified: online, batch, \\nor streaming predictions. In addition to serving predictions, the serving runtime can generate model explana -\\ntions and capture serving logs to be used by the continuous monitoring process. \\n6. The continuous monitoring process monitors the model for predictive effectiveness and service. The primary \\nconcern of effectiveness performance monitoring is detecting model decay—for example, data and concept \\ndrift. The model deployment can also be monitored for efficiency metrics like latency, throughput, hardware \\nresource utilization, and execution errors. \\nMLOps capabilities\\nTo effectively implement the key MLOps processes outlined in the previous section, organizations need to establish a \\nset of core technical capabilities. These capabilities can be provided by a single integrated ML platform. Alternative -\\nly, they can be created by combining vendor tools that each are best suited to particular tasks, developed as custom \\nservices, or created as a combination of these approaches.\\nIn most cases, the processes are deployed in stages rather than all at once in a single deployment. An organization’s \\nplan for adopting these processes and capabilities should align with business priorities and with the organization’s \\ntechnical and skills maturity. For example, many organizations start by focusing on the processes for ML develop -\\nment, model deployment, and prediction serving. For these organizations, continuous training and continuous moni -\\ntoring might not be necessary if they are piloting a relatively small number of ML systems.\\nFigure 4 shows the core set of technical capabilities that are generally required for MLOps. They are abstracted as \\nfunctional components that can have many-to-many mappings to specific products and technologies.\\n\\n10\\nSome foundational capabilities are required in order to support any IT workload, such as a reliable, scalable, and \\nsecure compute infrastructure. Most organizations already have investments in these capabilities and can benefit by \\ntaking advantage of them for ML workflows. Such capabilities might span multiple clouds, or even operate partially \\non-premises. Ideally, this would include advanced capabilities such as specialized ML accelerators.\\nIn addition, an organization needs standardized configuration management and CI/CD capabilities to build, test, \\nrelease, and operate software systems rapidly and reliably, including ML systems.\\nOn top of these foundational capabilities is a set of core MLOps capabilities. These include experimentation, data \\nprocessing, model training, model evaluation, model serving, online experimentation, model monitoring, ML pipeline, \\nand model registry. Finally, two cross-cutting capabilities that enable integration and interaction are an ML metadata \\nand artifact repository and an ML dataset and feature repository.\\nFigure 4.  Core MLOps technical capabilities\\n\\n11\\nThe following sections outline the characteristics of each of the MLOps capabilities.\\nExperimentation \\nThe experimentation capability lets your data scientists and ML researchers collaboratively perform exploratory data \\nanalysis, create prototype model architectures, and implement training routines. An ML environment should also let \\nthem write modular, reusable, and testable source code that is version controlled. Key functionalities in experimenta -\\ntion include the following:\\n• Provide notebook environments that are integrated with version control tools like Git.\\n• Track experiments, including information about the data, hyperparameters, and evaluation metrics for  \\nreproducibility and comparison.\\n• Analyze and visualize data and models.\\n• Support exploring datasets, finding experiments, and reviewing implementations.\\n• Integrate with other data services and ML services in your platform.\\nData processing\\nThe data processing capability lets you prepare and transform large amounts of data for ML at scale in ML develop -\\nment, in continuous training pipelines, and in prediction serving. Key functionalities in data processing include the \\nfollowing:\\n• Support interactive execution (for example, from notebooks) for quick experimentation and for long-running \\njobs in production.\\n• Provide data connectors to a wide range of data sources and services, as well as data encoders and  \\ndecoders for various data structures and formats.\\n• Provide both rich and efficient data transformations and ML feature engineering for structured (tabular) and \\nunstructured data (text, image, and so on).\\n• Support scalable batch and stream data processing for ML training and serving workloads.\\nModel training\\nThe model training capability lets you efficiently and cost-effectively run powerful algorithms for training ML models.')]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkZ-0e4y0HRM",
        "outputId": "161a74c3-b585-4732-ca3a-be689c1ce4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stuff_answer = stuff_chain(\n",
        "    {\n",
        "        'input_documents': docs,\n",
        "        'question': question,\n",
        "    },\n",
        "    return_only_outputs=True,\n",
        ")\n",
        "pprint(stuff_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkYXaIvp0Nmw",
        "outputId": "4d0307da-0f16-4ed7-8bcb-88e07fa40f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'output_text': 'Data management and feature management systems help mitigate '\n",
            "                'such issues by providing a unified repository for ML features '\n",
            "                'and datasets. As the diagram shows, the features and datasets '\n",
            "                'are created, discovered, and reused in different experiments. '\n",
            "                'Batch serving of the data is used for experimentation, '\n",
            "                'continuous training, and batch prediction, while online '\n",
            "                'serving of the data is used for real-time prediction use '\n",
            "                'cases.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HWa8FMsX1Cwg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}